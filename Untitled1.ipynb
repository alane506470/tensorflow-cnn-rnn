{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices=tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
      "WARNING:tensorflow:From D:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "0 0 loss 105.46023559570312\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[512,64,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MaxPoolGrad]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b40cf99002d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meager_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-b40cf99002d6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mloss_mean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mgrads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mD:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_MaxPoolGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    685\u001b[0m       \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"strides\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m       \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m       data_format=op.get_attr(\"data_format\"))\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mmax_pool_grad\u001b[1;34m(orig_input, orig_output, grad, ksize, strides, padding, data_format, name)\u001b[0m\n\u001b[0;32m   6615\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6616\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6617\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6618\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6619\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ANACONDA\\envs\\ML\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[512,64,16,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MaxPoolGrad]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_model():\n",
    "    vgg_layers=[\n",
    "        tf.keras.layers.Conv2D(64,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(64,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
    "    \n",
    "        tf.keras.layers.Conv2D(128,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(128,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
    "    \n",
    "        tf.keras.layers.Conv2D(128,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(128,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
    "    \n",
    "        tf.keras.layers.Conv2D(256,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(256,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(256,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
    "    \n",
    "        tf.keras.layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.Conv2D(512,kernel_size=[3,3],padding='same',activation=tf.nn.relu),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=[2,2],strides=2,padding='same'),\n",
    "    ]\n",
    "\n",
    "   \n",
    "    vgg_model=tf.keras.Sequential(vgg_layers)\n",
    "    vgg_model.build(input_shape=[None,32,32,3])\n",
    "    return vgg_model\n",
    "\n",
    "def create_fc():\n",
    "    fc_layers=[\n",
    "        tf.keras.layers.Dense(256,activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(256,activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(10,activation='softmax')\n",
    "    ]\n",
    "    fc_model=tf.keras.Sequential(fc_layers)\n",
    "    fc_model.build(input_shape=[None,512])\n",
    "    return fc_model\n",
    "def feature_scale(inputdata,outputdata):\n",
    "    x=tf.cast(inputdata,tf.float32)/255.0\n",
    "    y=tf.cast(outputdata,tf.int32)\n",
    "    return x,y\n",
    "#在Eager模式中，创建Iterator的方式有所不同。\n",
    "#是通过tfe.Iterator(dataset)的形式直接创建Iterator并迭代\n",
    "#。迭代时可以直接取出值，不需要使用sess.run()：\n",
    "#它的真正作用是切分传入Tensor的第一个维度，生成相应的dataset。\n",
    "def eager_iterator(inputdata,outputdata):\n",
    "    data = tf.data.Dataset.from_tensor_slices((inputdata,outputdata))\n",
    "    data = data.map(feature_scale).shuffle(10000).batch(512)\n",
    "    return data\n",
    "\n",
    "def train(data):\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n",
    "    for i in range(1):\n",
    "        for step,(x,y)in enumerate(data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                vggmodel=create_model()\n",
    "                fcmodel=create_fc()\n",
    "                variables = vggmodel.variables+fcmodel.variables\n",
    "                vgg_output=vggmodel(x)\n",
    "                output=fcmodel(vgg_output)\n",
    "                loss=tf.losses.categorical_crossentropy(y,output)\n",
    "                loss_mean=tf.reduce_mean(loss)\n",
    "            grads=tape.gradient(loss,variables)\n",
    "            optimizer.apply_gradients(zip(grads,variables))\n",
    "            if step%100 ==0:\n",
    "                print(i,step,'loss',float(loss_mean))\n",
    "        total_loss = 0\n",
    "        total_num=0\n",
    "        data_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "        data_test = data_test.map(feature_scale).batch(512) \n",
    "        for x,y in data_test:\n",
    "            logits = vggmodel(x)\n",
    "            #print(\"logits1:{0}\",logits.shape)\n",
    "            logits = tf.reshape(logits,[-1,512])\n",
    "            #print(\"logits1:{0}\",logits.shape)\n",
    "            logits = fcmodel(logits)\n",
    "            #print(\"logits2:{0}\",logits.shape)\n",
    "            #歸一化\n",
    "            prob = tf.nn.softmax(logits,axis=1)\n",
    "            #print(\"softmax {0}\",prob)\n",
    "            #返回類別索引\n",
    "            pred = tf.argmax(prob,axis=1)\n",
    "            #print(\"argmax {0}\",pred)\n",
    "            pred = tf.cast(pred,dtype=tf.int32)\n",
    "            correct = tf.equal(pred,y)\n",
    "            print(\"correct {0}\",correct)\n",
    "            result = tf.reduce_sum(tf.cast(correct,dtype=tf.int32))\n",
    "            total_loss += int(result)\n",
    "            total_num += x.shape[0]\n",
    "            acc = total_loss/total_num\n",
    "            print('acc {0}',acc)\n",
    "            \n",
    "def test_accuracy(data_test):\n",
    "    total_loss = 0\n",
    "    total_num=0\n",
    "    for x,y in data_test:\n",
    "        logits = vgg(x)\n",
    "        logits = tf.reshape(logits,[-1,512])\n",
    "        logits = fc(logits)\n",
    "        prob = tf.nn.softmax(logits,axis=1)\n",
    "        pred = tf.argmax(prob,axis=1)\n",
    "        pred = tf.cast(pred,dtype=tf.int32)\n",
    "        correct = tf.equal(pred,y)\n",
    "        result = tf.reduce_sum(tf.cast(correct,dtype=tf.int32))\n",
    "        total_loss += int(result)\n",
    "        total_num += x.shape[0]\n",
    "        acc = total_loss/total_num\n",
    "\n",
    "if __name__=='__main__':\n",
    "    (x,y),(x_test,y_test) =tf.keras.datasets.cifar10.load_data()\n",
    "    print(x.shape,x_test.shape)\n",
    "    data=eager_iterator(x,y)\n",
    "    train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    data_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))\n",
    "        data_test = data_test.map(feature_scale).batch(512) \n",
    "        for x,y in data_test:\n",
    "            logits = vgg(x)\n",
    "            print(\"logits1:{0}\",logits.shape)\n",
    "            logits = tf.reshape(logits,[-1,512])\n",
    "            print(\"logits1:{0}\",logits.shape)\n",
    "            logits = fc(logits)\n",
    "            print(\"logits2:{0}\",logits.shape)\n",
    "            #歸一化\n",
    "            prob = tf.nn.softmax(logits,axis=1)\n",
    "            print(\"softmax {0}\",prob)\n",
    "            #返回類別索引\n",
    "            pred = tf.argmax(prob,axis=1)\n",
    "            print(\"argmax {0}\",pred)\n",
    "            pred = tf.cast(pred,dtype=tf.int32)\n",
    "            correct = tf.equal(pred,y)\n",
    "            print(\"correct {0}\",correct)\n",
    "            result = tf.reduce_sum(tf.cast(correct,dtype=tf.int32))\n",
    "            total_loss += int(result)\n",
    "            total_num += x.shape[0]\n",
    "            acc = total_loss/total_num\n",
    "            print('acc {0}',acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
